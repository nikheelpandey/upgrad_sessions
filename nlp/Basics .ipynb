{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "governing-north",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The home side struck twice early in the morning session, leaving them 182 runs behind and with just two more wickets to get, with CricViz giving them a 47 per cent of winning the match at that point.\\n\\nBut when Root brought speedster Mark Wood into the attack, set fielders back on the rope and seemingly attempted to bounce out tail enders Jasprit Bumrah and Mohammed Shami, the match turned in India's favour.\\n\\nThe batting pair withstood the short-ball barrage and then flourished in a match-changing 89-run partnership that ruined any hopes England had of winning the game and paved the way for a famous Indian victory on 16/8/2021.\\n\\nEngland's tactics, particularly to Bumrah, were seemingly in response to the Indian quick's own short-ball assault to Jimmy Anderson late on day three, which drew the ire of the English veteran.\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "\n",
    "\n",
    "text = \"\"\"The home side struck twice early in the morning session, leaving them 182 runs behind and with just two more wickets to get, with CricViz giving them a 47 per cent of winning the match at that point.\n",
    "\n",
    "But when Root brought speedster Mark Wood into the attack, set fielders back on the rope and seemingly attempted to bounce out tail enders Jasprit Bumrah and Mohammed Shami, the match turned in India's favour.\n",
    "\n",
    "The batting pair withstood the short-ball barrage and then flourished in a match-changing 89-run partnership that ruined any hopes England had of winning the game and paved the way for a famous Indian victory on 16/8/2021.\n",
    "\n",
    "England's tactics, particularly to Bumrah, were seemingly in response to the Indian quick's own short-ball assault to Jimmy Anderson late on day three, which drew the ire of the English veteran.\"\"\"\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "owned-chorus",
   "metadata": {},
   "source": [
    "# 1. Sentence Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ancient-saint",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The home side struck twice early in the morning session, leaving them 182 runs behind and with just two more wickets to get, with CricViz giving them a 47 per cent of winning the match at that point.\n",
      "\n",
      "But when Root brought speedster Mark Wood into the attack, set fielders back on the rope and seemingly attempted to bounce out tail enders Jasprit Bumrah and Mohammed Shami, the match turned in India's favour.\n",
      "\n",
      "The batting pair withstood the short-ball barrage and then flourished in a match-changing 89-run partnership that ruined any hopes England had of winning the game and paved the way for a famous Indian victory on 16/8/2021.\n",
      "\n",
      "England's tactics, particularly to Bumrah, were seemingly in response to the Indian quick's own short-ball assault to Jimmy Anderson late on day three, which drew the ire of the English veteran.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentences = nltk.sent_tokenize(text)\n",
    "for sentence in sentences:\n",
    "    print(sentence)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "innocent-selling",
   "metadata": {},
   "source": [
    "# 2. Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "apart-routine",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'home', 'side', 'struck', 'twice', 'early', 'in', 'the', 'morning', 'session', ',', 'leaving', 'them', '182', 'runs', 'behind', 'and', 'with', 'just', 'two', 'more', 'wickets', 'to', 'get', ',', 'with', 'CricViz', 'giving', 'them', 'a', '47', 'per', 'cent', 'of', 'winning', 'the', 'match', 'at', 'that', 'point', '.']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_sentence = sentences[0]\n",
    "\n",
    "words = nltk.word_tokenize(sample_sentence)\n",
    "print(words)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neutral-saver",
   "metadata": {},
   "source": [
    "# 3. Text Lemmatization and Stemming "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "according-distinction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmer: went\n",
      "Lemmatizer: go\n",
      "\n",
      "Stemmer: drove\n",
      "Lemmatizer: drive\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "The goal of both stemming and lemmatization is to reduce inflectional forms \n",
    "and sometimes derivationally related forms of a word to a common base form.\n",
    "\"\"\"\n",
    "\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def compare_stemmer_and_lemmatizer(stemmer, lemmatizer, word, pos):\n",
    "    print(\"Stemmer:\", stemmer.stem(word))\n",
    "    print(\"Lemmatizer:\", lemmatizer.lemmatize(word, pos))\n",
    "    print()\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "compare_stemmer_and_lemmatizer(stemmer, lemmatizer, word = \"went\", pos = wordnet.VERB)\n",
    "compare_stemmer_and_lemmatizer(stemmer, lemmatizer, word = \"drove\", pos = wordnet.VERB)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modified-particle",
   "metadata": {},
   "source": [
    "# 4 Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "sharp-bible",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Stop words are words which are filtered out before or after processing of text. \n",
    "When applying machine learning to text, these words can add a lot of noise. \n",
    "That’s why we want to remove these irrelevant words.\n",
    "\"\"\"\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "interested-nickel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "England's tactics, particularly to Bumrah, were seemingly in response to the Indian quick's own short-ball assault to Jimmy Anderson late on day three, which drew the ire of the English veteran.\n",
      "\n",
      "['England', \"'s\", 'tactics', ',', 'particularly', 'Bumrah', ',', 'seemingly', 'response', 'Indian', 'quick', \"'s\", 'short-ball', 'assault', 'Jimmy', 'Anderson', 'late', 'day', 'three', ',', 'drew', 'ire', 'English', 'veteran', '.']\n"
     ]
    }
   ],
   "source": [
    "print(sentence)\n",
    "print()\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "words = nltk.word_tokenize(sentence)\n",
    "without_stop_words = [word for word in words if not word in stop_words]\n",
    "print(without_stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "working-roberts",
   "metadata": {},
   "source": [
    "# 5. Regular Expressions\n",
    "\n",
    "\n",
    "### A regular expression, regex, or regexp is a sequence of characters that define a search pattern. Let’s see some basics.\n",
    "\n",
    "\n",
    "- . - match any character except newline\n",
    "- \\w - match word\n",
    "- \\d - match digit\n",
    "- \\s - match whitespace\n",
    "- \\W - match not word\n",
    "- \\D - match not digit\n",
    "- \\S - match not whitespace\n",
    "- [abc] - match any of a, b, or c\n",
    "- [^abc] - not match a, b, or c\n",
    "- [a-g] - match a character between a & g\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "intense-costs",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:  The batting pair withstood the short-ball barrage and then flourished in a match-changing 89-run partnership that ruined any hopes England had of winning the game and paved the way for a famous Indian victory on 16/8/2021.\n",
      "\n",
      "\n",
      "\n",
      "Regex filter:  The batting pair withstood the short-ball barrage and then flourished in a match-changing ### -run partnership that ruined any hopes England had of winning the game and paved the way for a famous Indian victory on ### /8/### ### .\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# print(sentences)\n",
    "sentence = sentences[2]\n",
    "\n",
    "print(\"Sentence: \", sentence)\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "pattern = r\"\\d\\d\"\n",
    "\n",
    "\n",
    "print(\"Regex filter: \",re.sub(pattern, \"### \", sentence))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recent-concert",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r\"89-\"#removes , and '\n",
    "\n",
    "\n",
    "print(\"Regex filter: \",re.sub(pattern, \"eighty-nine \", sentence))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "congressional-situation",
   "metadata": {},
   "source": [
    "# 6. Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "convenient-batman",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The home side struck twice early in the morning session, leaving them 182 runs behind and with just two more wickets to get, with CricViz giving them a 47 per cent of winning the match at that point.',\n",
       " '',\n",
       " \"But when Root brought speedster Mark Wood into the attack, set fielders back on the rope and seemingly attempted to bounce out tail enders Jasprit Bumrah and Mohammed Shami, the match turned in India's favour.\",\n",
       " '',\n",
       " 'The batting pair withstood the short-ball barrage and then flourished in a match-changing 89-run partnership that ruined any hopes England had of winning the game and paved the way for a famous Indian victory.',\n",
       " '',\n",
       " \"England's tactics, particularly to Bumrah, were seemingly in response to the Indian quick's own short-ball assault to Jimmy Anderson late on day three, which drew the ire of the English veteran.\"]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"simple_text.txt\", \"r\") as file:\n",
    "    documents = file.read().splitlines()\n",
    "    \n",
    "    \n",
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "widespread-airfare",
   "metadata": {},
   "source": [
    "The bag-of-words model is a popular and simple feature extraction technique used when we work with text. It describes the occurrence of each word within a document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "together-surgery",
   "metadata": {},
   "source": [
    "To use this model, we need to:\n",
    "- Design a vocabulary of known words (also called tokens)\n",
    "- Choose a measure of the presence of known words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "certified-phenomenon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>182</th>\n",
       "      <th>47</th>\n",
       "      <th>89</th>\n",
       "      <th>and</th>\n",
       "      <th>anderson</th>\n",
       "      <th>any</th>\n",
       "      <th>assault</th>\n",
       "      <th>at</th>\n",
       "      <th>attack</th>\n",
       "      <th>attempted</th>\n",
       "      <th>...</th>\n",
       "      <th>victory</th>\n",
       "      <th>way</th>\n",
       "      <th>were</th>\n",
       "      <th>when</th>\n",
       "      <th>which</th>\n",
       "      <th>wickets</th>\n",
       "      <th>winning</th>\n",
       "      <th>with</th>\n",
       "      <th>withstood</th>\n",
       "      <th>wood</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7 rows × 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   182  47  89  and  anderson  any  assault  at  attack  attempted  ...  \\\n",
       "0    1   1   0    1         0    0        0   1       0          0  ...   \n",
       "1    0   0   0    0         0    0        0   0       0          0  ...   \n",
       "2    0   0   0    2         0    0        0   0       1          1  ...   \n",
       "3    0   0   0    0         0    0        0   0       0          0  ...   \n",
       "4    0   0   1    2         0    1        0   0       0          0  ...   \n",
       "5    0   0   0    0         0    0        0   0       0          0  ...   \n",
       "6    0   0   0    0         1    0        1   0       0          0  ...   \n",
       "\n",
       "   victory  way  were  when  which  wickets  winning  with  withstood  wood  \n",
       "0        0    0     0     0      0        1        1     2          0     0  \n",
       "1        0    0     0     0      0        0        0     0          0     0  \n",
       "2        0    0     0     1      0        0        0     0          0     1  \n",
       "3        0    0     0     0      0        0        0     0          0     0  \n",
       "4        1    1     0     0      0        0        1     0          1     0  \n",
       "5        0    0     0     0      0        0        0     0          0     0  \n",
       "6        0    0     1     0      1        0        0     0          0     0  \n",
       "\n",
       "[7 rows x 101 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the libraries we need\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Step 2. Design the Vocabulary\n",
    "# The default token pattern removes tokens of a single character. That's why we don't have the \"I\" and \"s\" tokens in the output\n",
    "count_vectorizer = CountVectorizer()\n",
    "\n",
    "# Step 3. Create the Bag-of-Words Model\n",
    "bag_of_words = count_vectorizer.fit_transform(documents)\n",
    "\n",
    "# Show the Bag-of-Words Model as a pandas DataFrame\n",
    "feature_names = count_vectorizer.get_feature_names()\n",
    "pd.DataFrame(bag_of_words.toarray(), columns = feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "athletic-singer",
   "metadata": {},
   "source": [
    "# 7. TF-IDF\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "streaming-drain",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "values = tfidf_vectorizer.fit_transform(documents)\n",
    "\n",
    "# Show the Model as a pandas DataFrame\n",
    "feature_names = tfidf_vectorizer.get_feature_names()\n",
    "pd.DataFrame(values.toarray(), columns = feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blank-louisville",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
